{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Du6rimrD4gbN"
   },
   "source": [
    "<h1><b>HW1: Thu thập và tiền xử lý dữ liệu</b></h1>\n",
    "\n",
    "Họ tên: Trần Hoàng Anh Phi\n",
    "\n",
    "MSSV: 20120158\n",
    "\n",
    "<hr />\n",
    "\n",
    "<h2><b>Cách làm và nộp bài</b></h2>\n",
    "<b>Cách làm bài</b>\n",
    "\n",
    "Các bạn sẽ làm trực tiếp trên file notebook này. Đầu tiên, bạn điền họ tên và MSSV của mình ở bên trên. Trong file, bạn sẽ làm ở những phần có ghi là:\n",
    "\n",
    "<i style=\"background-color:tomato\"><FONT COLOR=\"green\"># YOUR CODE HERE</FONT></i>\n",
    "\n",
    "<FONT COLOR=\"green\">raise</FONT> NotImplementedError()\n",
    "\n",
    "Tất nhiên, khi làm bạn sẽ xóa dòng <b>raise NotImplementedError()</b> đi. Đối với những phần yêu cầu làm thì thường ở ngay phía dưới sẽ có một (hoặc một số) cell chứa các bộ test để giúp bạn biết đã code đúng hay chưa. Nếu chạy cell này không có lỗi gì thì có nghĩa là qua được các bộ test và ngược lại, nếu không qua được test thì là code sai, nhưng nếu qua được test thì chưa chắc đã đúng.\n",
    "</br></br>\n",
    "Trong khi làm bài, bạn có thể cho in ra màn hình, tạo thêm các cell để test. Tuy nhiên, khi nộp bài thì code của các bạn sẽ được chạy lại, do đó phải xóa các cell mà bạn đã tạo ra trước đó đi. Ngoài ra, bạn lưu ý là tuyệt đối không được tự tiện xóa các cell hay sửa code của Thầy (trừ những chỗ được phép sửa như đã đề cập ở trên).\n",
    "</br></br>\n",
    "Trong khi làm bài, thương xuyên lưu lại bài của bạn để tránh mất mát thông tin. Ngoài ra, chúng ta cần hết sức lưu ý là việc đạo văn là bị nghiêm cấm và sẽ bị xử lý đối với các bài làm giống nhau. Mục tiêu chính của chúng ta là học một cách chân thật, các bạn có thể tham khảo ý tưởng từ bạn bè của bạn hay từ các nguồn tham khảo khác trên mạng. Tuy nhiên, phần bài làm cuối cùng vẫn phải là của bạn dựa trên những sự hiểu biết thực sự. Khi tham khảo các nguồn trên mạng thì bạn cũng nên ghi rõ nguồn đó trong bài làm của mình. <font color='red'>Trong trường hợp các bạn vi phạm những điều mình đã đề cập ở trên thì sẽ bị 0 điểm cho toàn bộ môn học này.</font>\n",
    "\n",
    "<h5><b>Cách nộp bài</h5></b>\n",
    "\n",
    "Khi chấm bài, mình sẽ chọn Kernel - Restart & Run All để chạy lại toàn bộ code của các bạn từ ban đầu trên google colab. Do đó, để đảm bảo code các bạn không bị lỗi, các bạn nên chạy thử như mình chạy để đảm bảo mọi thứ diễn ra như mong đợi. \n",
    "\n",
    "Sau đó, bạn tạo thư mục nộp bài theo các yêu cầu sau:\n",
    "* Đầu tiên, các bạn tạo một thư mục MSSV (vd, nếu bạn có MSSV là 123456 thì bạn đặt tên thư mục là 123456)\n",
    "* Trong thư mục, MSSV vừa tạo, bạn bỏ file notebook bài làm của bạn vào cùng với file project được tạo bởi thư viện scrapy (file project khi tạo này các bạn có thể đặt bất kỳ tên gì)\n",
    "* Cuối cùng, bạn nén thư mục MSSV này lại và nộp ở link trên moodle. Đuôi của file nén phải là .zip (chứ không được .rar hay gì khác).\n",
    "\n",
    "<font color='red'>Bạn lưu ý tuân thủ chính xác qui định nộp bài ở trên, nếu nộp sai quy định sẽ bị trừ nửa số điểm cho bài làm của mình.</font>\n",
    "</br></br>\n",
    "<hr />\n",
    "</br>\n",
    "<h2><b>Import</b></h2>\n",
    "\n",
    "Trong bài thực hành này vì dữ liệu thu thập của chúng ta sẽ rất lớn, nên mình sẽ giúp các bạn thu thập dữ liệu được nhanh chóng bằng cách sử dụng một thư viện có tên scrapy. Vì thư viện này không có sẵn nên các bạn cần cài đặt trước khi sử dụng."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "7ZL9mrLkc3Dp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scrapy in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.6.3)\n",
      "Requirement already satisfied: cssselect>=0.9.1 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scrapy) (1.1.0)\n",
      "Requirement already satisfied: queuelib>=1.4.2 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scrapy) (1.6.2)\n",
      "Requirement already satisfied: Twisted>=18.9.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scrapy) (22.8.0)\n",
      "Requirement already satisfied: itemloaders>=1.0.1 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scrapy) (1.0.6)\n",
      "Requirement already satisfied: zope.interface>=5.0.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scrapy) (5.5.0)\n",
      "Requirement already satisfied: itemadapter>=0.1.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scrapy) (0.7.0)\n",
      "Requirement already satisfied: tldextract in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scrapy) (3.4.0)\n",
      "Requirement already satisfied: service-identity>=18.1.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scrapy) (21.1.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scrapy) (63.2.0)\n",
      "Requirement already satisfied: cryptography>=3.3 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scrapy) (38.0.2)\n",
      "Requirement already satisfied: pyOpenSSL>=21.0.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scrapy) (22.1.0)\n",
      "Requirement already satisfied: parsel>=1.5.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scrapy) (1.6.0)\n",
      "Requirement already satisfied: PyDispatcher>=2.0.5 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scrapy) (2.0.6)\n",
      "Requirement already satisfied: w3lib>=1.17.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scrapy) (2.0.1)\n",
      "Requirement already satisfied: lxml>=3.5.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scrapy) (4.9.1)\n",
      "Requirement already satisfied: protego>=0.1.15 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scrapy) (0.2.1)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from cryptography>=3.3->scrapy) (1.15.1)\n",
      "Requirement already satisfied: jmespath>=0.9.5 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from itemloaders>=1.0.1->scrapy) (1.0.1)\n",
      "Requirement already satisfied: six>=1.6.0 in c:\\users\\admin\\appdata\\roaming\\python\\python310\\site-packages (from parsel>=1.5.0->scrapy) (1.16.0)\n",
      "Requirement already satisfied: pyasn1-modules in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from service-identity>=18.1.0->scrapy) (0.2.8)\n",
      "Requirement already satisfied: pyasn1 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from service-identity>=18.1.0->scrapy) (0.4.8)\n",
      "Requirement already satisfied: attrs>=19.1.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from service-identity>=18.1.0->scrapy) (22.1.0)\n",
      "Requirement already satisfied: constantly>=15.1 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from Twisted>=18.9.0->scrapy) (15.1.0)\n",
      "Requirement already satisfied: twisted-iocpsupport<2,>=1.0.2 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from Twisted>=18.9.0->scrapy) (1.0.2)\n",
      "Requirement already satisfied: incremental>=21.3.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from Twisted>=18.9.0->scrapy) (21.3.0)\n",
      "Requirement already satisfied: hyperlink>=17.1.1 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from Twisted>=18.9.0->scrapy) (21.0.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.5 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from Twisted>=18.9.0->scrapy) (4.4.0)\n",
      "Requirement already satisfied: Automat>=0.8.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from Twisted>=18.9.0->scrapy) (20.2.0)\n",
      "Requirement already satisfied: requests-file>=1.4 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tldextract->scrapy) (1.5.1)\n",
      "Requirement already satisfied: requests>=2.1.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tldextract->scrapy) (2.28.1)\n",
      "Requirement already satisfied: filelock>=3.0.8 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tldextract->scrapy) (3.8.0)\n",
      "Requirement already satisfied: idna in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tldextract->scrapy) (3.4)\n",
      "Requirement already satisfied: pycparser in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from cffi>=1.12->cryptography>=3.3->scrapy) (2.21)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.1.0->tldextract->scrapy) (2.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.1.0->tldextract->scrapy) (1.26.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.1.0->tldextract->scrapy) (2022.9.24)\n",
      "Requirement already satisfied: spider3 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install scrapy\n",
    "!pip install spider3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Nah1kqOV74Wk"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "import scrapy\n",
    "from pandas.testing import assert_frame_equal # to compare two dataframes\n",
    "# YOUR CODE HERE (OPTION) \n",
    "# Nếu cần các thư viện khác thì bạn có thể import thêm tại đây\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CADRNR64WnmB"
   },
   "source": [
    "Như bạn đã biết, World Cup là một giải bóng đá lớn nhất thế giới được tổ chức mỗi 4 năm 1 lần. Vì cuối tháng 11 này kỳ World Cup 2022 sẽ diễn ra, do đó ở Lab 1 này, chúng ta sẽ khởi động môn học bằng việc thu thập dữ liệu của các cầu thủ bóng đá. <b>SoFIFA</b> (https://sofifa.com/) là một trang web lưu trữ dữ liệu của các cầu thủ trong trò chơi bóng đá nổi tiếng FIFA 23 mà có các chỉ số phản ánh gần đúng với phong độ của các cầu thủ bóng đá ngoài đời. Trong phần này, nhiệm vụ đầu tiên của bạn là sẽ cần thu thập ID của các cầu thủ. Mình có check \"Terms of Service\" của SoFIFA thì không thấy có mục nào nói là cấm parse HTML, vì vậy với mục đích học thì chắc là không có vấn đề gì lớn, miễn là chương trình của bạn đừng \"hit\" trang web quá nhiều lần trong một khoảng thời gian ngắn thì sẽ không có vấn đề gì. \n",
    "</br></br>\n",
    "Công việc cụ thể của bạn ở phần đầu tiên này là sẽ cần viết class <b>collect_player_url()</b> ở bên dưới. Vì dữ liệu lúc sau các bạn cần thu thập sẽ rất lớn nên ở đây, chúng ta sẽ xài một thư viên có tên scrapy để có thể thu thập dữ liệu lớn được nhanh chóng hơn. Về cách sử dụng thư viện này thì bạn có thể tham khảo thêm tại trang web sau: https://docs.scrapy.org/en/latest/intro/tutorial.html.\n",
    "</br></br>\n",
    "<h2><b>Tạo một project mới với scrapy</b><h2>\n",
    "\n",
    "Để sử dụng thư viện scrapy sau khi cài đặt xong, các bạn sẽ gọi câu lệnh như bên dưới để bắt đầu tạo một project mới với scrapy với tên gọi là <b>\"fifa_crawler\"</b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "6ndHFd1lWnUj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Scrapy project 'fifa_crawler', using template directory 'C:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scrapy\\templates\\project', created in:\n",
      "    D:\\Downloads\\nam 3\\Nhap mon KHDL\\Lab01_DL211122\\fifa_crawler\n",
      "\n",
      "You can start your first spider with:\n",
      "    cd fifa_crawler\n",
      "    scrapy genspider example example.com\n"
     ]
    }
   ],
   "source": [
    "!scrapy startproject fifa_crawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "TyLNgVqWvt3Z"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Downloads\\nam 3\\Nhap mon KHDL\\Lab01_DL211122\\fifa_crawler\\fifa_crawler\n"
     ]
    }
   ],
   "source": [
    "cd fifa_crawler/fifa_crawler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LtjRbH9NmmI8"
   },
   "source": [
    "Sau khi tạo xong project với scrapy, vì thu thập dữ liệu với thư viện này không cho phép xài notebook trực tiếp nên các bạn sau khi hoàn thành xong class <b>collect_player_url(scrapy.Spider)</b> ở bên dưới, các bạn cần tạo một file có tên <b>collect_players_urls.py</b> vào đường dẫn sau <b>fifa_crawler/fifa_crawler/spiders/collect_players_urls.py</b>. Ngoài ra, để tránh việc <b>hit</b> trang web quá nhiều lần, thay vì thu thập toàn bộ ID các cầu thủ của thì các bạn sẽ chỉ cần thu thập 720 ID cầu thủ. Để cho dễ dàng, mình đã để sẵn 1 đường dẫn urls chứa định dạng offset (từng trang của các cầu thủ với mỗi trang chứa 60 cầu thủ khác nhau) để các bạn có thể dễ dàng chuyển trang mới trong lúc thu thập."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nsIYPdgWeJ0z"
   },
   "outputs": [],
   "source": [
    "import scrapy\n",
    "\n",
    "class collect_player_url(scrapy.Spider):\n",
    "  name='players_urls' \n",
    "  \n",
    "  def start_requests(self):\n",
    "    urls = ['https://sofifa.com/players?col=oa&sort=desc&offset=0']\n",
    "    # YOUR CODE HERE\n",
    "    return scrapy.request(urls, callback = self.parse)\n",
    "  def parse(self, response):\n",
    "    # YOUR CODE HERE\n",
    "    for href in response.xpath(\"//tbody/tr/td[2]/a[1]/@href\"):\n",
    "        yield response.follow(href, self.parse_player)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ShUFd8SmfCgf"
   },
   "source": [
    "Sau khi đã viết xong class trên, và đã tạo file <b>collect_players_urls.py</b> trong đường dẫn đã đề cập, bạn sẽ tiến hành gọi lệnh như bên dưới để thu thập ID của các cầu thủ bóng đá và lưu vào một file có tên <b>players_urls.json</b> mà chứa trong thư mục dataset. Thư mục dataset này bạn không cần tạo mà khi thu thập dữ liệu scrapy sẽ tự động tạo cho bạn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "kLoJ44s6e9NG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scrapy 2.6.3 - no active project\n",
      "\n",
      "Unknown command: crawl\n",
      "\n",
      "Use \"scrapy\" to see available commands\n"
     ]
    }
   ],
   "source": [
    "!scrapy crawl players_urls -o dataset/players_urls.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "caESkJGPopG9"
   },
   "source": [
    "Sau khi đã có danh sách 720 ID của các cầu thủ đã thu thập từ trang web SoFIFA, bạn sẽ tiến hành thu thập dữ liệu cụ thể của từng cầu thủ ứng với các ID này bằng cách hoàn thành class <b>collect_player_info(scrapy.Spider)</b> như bên dưới. Các bạn cũng lưu ý lại là như mình đã đề cập, việc sử dụng scrapy trực tiếp notebook là không được mà chúng ta sẽ cần tạo tiếp một file mới có tên <b>collect_players_info.py</b> vào cùng đường dẫn như file <b>collect_players_urls.py</b>. Ở đây, cũng để cho tiện thì mình cũng để cho các bạn 1 đường dẫn url mới với ID 231747 ứng với ID đầu tiên mà chúng ta đã thu thập ở bước trước. Nhiệm vụ của các bạn là dựa vào url này để tiếp tục hoàn thành việc parse HTML và thu thập các thông tin chi tiết của ID này (chúng ta cũng làm tương tự cho 719 ID còn lại). Về chi tiết tất cả các dữ liệu các bạn cần thu thập thì mình có để sẵn 1 file test có tên <b>players_info.json</b> là file đã được mình thu thập về thông tin chi tiết của 720 cầu thủ cho các bạn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wHicINKmvDvC",
    "outputId": "e6693593-7eb2-44e4-d8e5-d38b0993ba83"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File /content/players_info.json does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\Downloads\\Nam 3\\Nhap mon KHDL\\Lab01_DL211122\\Lab01.ipynb Cell 12\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Downloads/Nam%203/Nhap%20mon%20KHDL/Lab01_DL211122/Lab01.ipynb#X14sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m df_test \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_json(\u001b[39m'\u001b[39;49m\u001b[39m/content/players_info.json\u001b[39;49m\u001b[39m'\u001b[39;49m, encoding\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mutf-8-sig\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Downloads/Nam%203/Nhap%20mon%20KHDL/Lab01_DL211122/Lab01.ipynb#X14sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m df_test\u001b[39m.\u001b[39miloc[\u001b[39m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\util\\_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[39m=\u001b[39m new_arg_value\n\u001b[1;32m--> 211\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\util\\_decorators.py:317\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    311\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[0;32m    312\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    313\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39marguments),\n\u001b[0;32m    314\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[0;32m    315\u001b[0m         stacklevel\u001b[39m=\u001b[39mfind_stack_level(inspect\u001b[39m.\u001b[39mcurrentframe()),\n\u001b[0;32m    316\u001b[0m     )\n\u001b[1;32m--> 317\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\json\\_json.py:733\u001b[0m, in \u001b[0;36mread_json\u001b[1;34m(path_or_buf, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, numpy, precise_float, date_unit, encoding, encoding_errors, lines, chunksize, compression, nrows, storage_options)\u001b[0m\n\u001b[0;32m    730\u001b[0m \u001b[39mif\u001b[39;00m convert_axes \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m orient \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtable\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    731\u001b[0m     convert_axes \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m--> 733\u001b[0m json_reader \u001b[39m=\u001b[39m JsonReader(\n\u001b[0;32m    734\u001b[0m     path_or_buf,\n\u001b[0;32m    735\u001b[0m     orient\u001b[39m=\u001b[39;49morient,\n\u001b[0;32m    736\u001b[0m     typ\u001b[39m=\u001b[39;49mtyp,\n\u001b[0;32m    737\u001b[0m     dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[0;32m    738\u001b[0m     convert_axes\u001b[39m=\u001b[39;49mconvert_axes,\n\u001b[0;32m    739\u001b[0m     convert_dates\u001b[39m=\u001b[39;49mconvert_dates,\n\u001b[0;32m    740\u001b[0m     keep_default_dates\u001b[39m=\u001b[39;49mkeep_default_dates,\n\u001b[0;32m    741\u001b[0m     numpy\u001b[39m=\u001b[39;49mnumpy,\n\u001b[0;32m    742\u001b[0m     precise_float\u001b[39m=\u001b[39;49mprecise_float,\n\u001b[0;32m    743\u001b[0m     date_unit\u001b[39m=\u001b[39;49mdate_unit,\n\u001b[0;32m    744\u001b[0m     encoding\u001b[39m=\u001b[39;49mencoding,\n\u001b[0;32m    745\u001b[0m     lines\u001b[39m=\u001b[39;49mlines,\n\u001b[0;32m    746\u001b[0m     chunksize\u001b[39m=\u001b[39;49mchunksize,\n\u001b[0;32m    747\u001b[0m     compression\u001b[39m=\u001b[39;49mcompression,\n\u001b[0;32m    748\u001b[0m     nrows\u001b[39m=\u001b[39;49mnrows,\n\u001b[0;32m    749\u001b[0m     storage_options\u001b[39m=\u001b[39;49mstorage_options,\n\u001b[0;32m    750\u001b[0m     encoding_errors\u001b[39m=\u001b[39;49mencoding_errors,\n\u001b[0;32m    751\u001b[0m )\n\u001b[0;32m    753\u001b[0m \u001b[39mif\u001b[39;00m chunksize:\n\u001b[0;32m    754\u001b[0m     \u001b[39mreturn\u001b[39;00m json_reader\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\json\\_json.py:818\u001b[0m, in \u001b[0;36mJsonReader.__init__\u001b[1;34m(self, filepath_or_buffer, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, numpy, precise_float, date_unit, encoding, lines, chunksize, compression, nrows, storage_options, encoding_errors)\u001b[0m\n\u001b[0;32m    815\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlines:\n\u001b[0;32m    816\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mnrows can only be passed if lines=True\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 818\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_data_from_filepath(filepath_or_buffer)\n\u001b[0;32m    819\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_preprocess_data(data)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\json\\_json.py:874\u001b[0m, in \u001b[0;36mJsonReader._get_data_from_filepath\u001b[1;34m(self, filepath_or_buffer)\u001b[0m\n\u001b[0;32m    866\u001b[0m     filepath_or_buffer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles\u001b[39m.\u001b[39mhandle\n\u001b[0;32m    867\u001b[0m \u001b[39melif\u001b[39;00m (\n\u001b[0;32m    868\u001b[0m     \u001b[39misinstance\u001b[39m(filepath_or_buffer, \u001b[39mstr\u001b[39m)\n\u001b[0;32m    869\u001b[0m     \u001b[39mand\u001b[39;00m filepath_or_buffer\u001b[39m.\u001b[39mlower()\u001b[39m.\u001b[39mendswith(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    872\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m file_exists(filepath_or_buffer)\n\u001b[0;32m    873\u001b[0m ):\n\u001b[1;32m--> 874\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mFileNotFoundError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFile \u001b[39m\u001b[39m{\u001b[39;00mfilepath_or_buffer\u001b[39m}\u001b[39;00m\u001b[39m does not exist\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    876\u001b[0m \u001b[39mreturn\u001b[39;00m filepath_or_buffer\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: File /content/players_info.json does not exist"
     ]
    }
   ],
   "source": [
    "df_test = pd.read_json('/content/players_info.json', encoding='utf-8-sig')\n",
    "df_test.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7vHUkFitpbpv"
   },
   "outputs": [],
   "source": [
    "import scrapy\n",
    "import json\n",
    "\n",
    "class collect_player_info(scrapy.Spider):\n",
    "  name='players_info'\n",
    "  \n",
    "  def __init__(self):\n",
    "    try:\n",
    "      with open('dataset/players_urls.json') as f:\n",
    "        self.players = json.load(f)\n",
    "      self.player_count = 1\n",
    "    except IOError:\n",
    "      print(\"File not found\")\n",
    "\n",
    "  def start_requests(self):\n",
    "    urls = ['https://sofifa.com/player/231747?units=mks']\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "  \n",
    "  def parse(self, response):\n",
    "      # YOUR CODE HERE\n",
    "      raise NotImplementedError()\n",
    "      \n",
    "      if self.player_count < len(self.players):\n",
    "        next_page_url = 'https://sofifa.com' + self.players[self.player_count]['player_url'] + '?units=mks'\n",
    "        self.player_count += 1\n",
    "        yield scrapy.Request(url=next_page_url, callback=self.parse) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WMtk8cmuqnXm"
   },
   "source": [
    "Sau khi đã hoàn thành class ở trên và lưu lại trong file <b>collect_players_info.py</b>, các bạn sẽ tiếp tục chạy câu lệnh bên dưới để thu thập thông tin chi tiết của toàn bộ 720 cầu thủ và xuất ra file <b>players_info.json</b> ở cùng đường dẫn dataset như trên."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZeWsqMVMpNpW"
   },
   "outputs": [],
   "source": [
    "!scrapy crawl players_info -o dataset/players_info.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "swqaBClCrg39"
   },
   "source": [
    "Sau khi đã có thông tin chi tiết của 720 cầu thủ, các bạn sẽ tiến hành đọc file <b>players_info.json</b> vào pandas với tên gọi <b>df_players_info</b> và kiểm tra lại với file của mình xem dữ liệu các bạn thu thập là khớp chưa. Nếu đã trùng khớp hai file thì bạn sẽ đến với bước tiền xử lý dữ liệu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "id": "vIzyLHd2rPwI"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "df_players_info = pd.read_json('fifa_crawler/dataset/players_info.json', encoding='utf-8-sig')\n",
    "df_test = pd.read_json('test/players_info.json', encoding='utf-8-sig')\n",
    "#df_diff = pd.concat([df_players_info,df_test]).drop_duplicates(keep=False)\n",
    "#print(df_diff)\n",
    "#assert df_test.iloc[0]==df_players_info.iloc[0]\n",
    "my_list = list(df_players_info.iloc[:,27])\n",
    "test_list = list(df_test.iloc[:,27])\n",
    "for i,j in enumerate(test_list):\n",
    "    if j not in my_list:\n",
    "        print(j,i)\n",
    "assert_frame_equal(df_players_info, df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "crik-hTWt1RH"
   },
   "source": [
    "<h2><b>Tiền xử lý dữ liệu</b></h2>\n",
    "\n",
    "<b>1)</b> Sau khi đã kiểm tra và khớp với dữ liệu từ file mình đã cung cấp, nhiệm vụ đầu tiên của các bạn trong bước tiền xử lý dữ liệu là cần phân tách các chỉ số cụ thể của mỗi cầu thủ. Như chúng ta thấy, ở mỗi cột <b>attacking, skill, movement</b> sẽ là một từ điển chứa chi tiết cụ thể từng chỉ số chi tiết ở trong đó. Do đó, chúng ta sẽ cần phân tách các từ điển này thành các cột cụ thể và nhiệm vụ khi này của bạn là hoàn thành hàm <b>split_players_info()</b> ở bên dưới rồi lưu lại vào một dataframe mới với tên <b>df_split_players_info</b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "id": "HFCvJ3r51ZHb"
   },
   "outputs": [],
   "source": [
    "def split_players_info():\n",
    "    # YOUR CODE HERE\n",
    "    #print(len(df_players_info.columns))\n",
    "    df_ = np.split(df_players_info,list(range(0,len(df_players_info.columns)+1)),axis=1)\n",
    "    df_split_players_info = pd.DataFrame(df_players_info.to_dict())\n",
    "    #print(df_players_info.to_dict())\n",
    "    my_keys = [\"attacking\",\"skill\",\"movement\",\"power\",\"mentality\",\"defending\",\"goalkeeping\"]\n",
    "\n",
    "    for i in my_keys:\n",
    "        df_split_players_info = pd.concat([df_split_players_info,df_split_players_info[i].apply(pd.Series)],axis=1)\n",
    "        df_split_players_info.drop(columns=i,inplace=True)\n",
    "    #print(df_players_info.columns,'-------')\n",
    "    #print(df_split_players_info.columns,'-------------')\n",
    "    df_split_players_info_test = pd.read_json('test/split_players_info.json', encoding='utf-8-sig')\n",
    "    #print(df_split_players_info_test.columns,'-------')\n",
    "    #print(len(df_split_players_info.columns))\n",
    "    return df_split_players_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "id": "Trd9o5bUEujU"
   },
   "outputs": [],
   "source": [
    "# TEST\n",
    "df_split_players_info = split_players_info()\n",
    "df_split_players_info_test = pd.read_json('test/split_players_info.json', encoding='utf-8-sig')\n",
    "assert_frame_equal(df_split_players_info.iloc[:, 23:], df_split_players_info_test.iloc[:, 23:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VQtEUA263lFE"
   },
   "source": [
    "<b>2)</b> Vì cột giá trị của cầu thủ (Value) và tiền lương (Wage) đang ở dạng viết tắt nên các bạn cần chuyển hai cột này về dạng số (float) bằng cách hoàn thành hàm <b>value_and_wage_to_float(col)</b>. Ví dụ, cầu thủ A có giá trị là €1M và tiền lương là €1K, hai cột này sau khi được tiền xử lý thì cầu thủ A sẽ có giá trị là 1000000.0 và tiền lương là 1000. Ngoài ra, bạn cần lưu ý kiểm tra các cột này có giá trị bị thiếu hay không, nếu có sẽ điền là 0.0 và ngược lại sẽ tiền xử lý như trên."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "id": "-w1ySIhU3J_O"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "def value_and_wage_to_float(col):\n",
    "    # YOUR CODE HERE\n",
    "    df_split_players_info[col]= df_split_players_info[col].str.replace('€','')\n",
    "    df_split_players_info[col] = df_split_players_info[col].replace({'[kK]$':'*1e3','[mM]$':'*1e6','': 0.0},regex=True).map(pd.eval)\n",
    "    print(df_split_players_info[col].to_list()[:5])\n",
    "    print(df_split_players_info[col].to_list()[-5:])\n",
    "value_and_wage_to_float(\"Wage\")\n",
    "value_and_wage_to_float(\"Value\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "id": "h6hcwHaZ4znE"
   },
   "outputs": [],
   "source": [
    "# TEST\n",
    "assert df_split_players_info['Wage'].to_list()[:5] == [230000.0, 350000.0, 420000.0, 450000.0, 195000.0]\n",
    "assert df_split_players_info['Wage'].to_list()[-5:] == [41000.0, 21000.0, 59000.0, 15000.0, 50000.0]\n",
    "assert df_split_players_info['Value'].to_list()[:5] == [190500000.0, 107500000.0, 84000000.0, 64000000.0, 54000000.0]\n",
    "assert df_split_players_info['Value'].to_list()[-5:] == [32000000.0, 9500000.0, 19500000.0, 22000000.0, 20000000.0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HpUsosnEDJia"
   },
   "source": [
    "<b>3)</b> Vì cột giá trị giải phóng hợp đồng (Release Clause) của cầu thủ đang ở dạng object, các bạn cần chuyển về dạng chuỗi và sau đó tiến hành tiền xử lý tương tự như hai cột <b>Value</b> và <b>Wage</b> ở trên bằng cách hoàn thành hàm <b>release_clause_to_float(col)</b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "id": "nDExKO517OuE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[366700000.0, 198900000.0, 172200000.0, 131199999.99999999, 99900000.0]\n",
      "<class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# def release_clause_to_float(col):\n",
    "#     # YOUR CODE HERE\n",
    "value_and_wage_to_float(\"Release Clause\")\n",
    "print(type(df_split_players_info[\"Release Clause\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "id": "Hkj6nUcEFI8b"
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32md:\\Downloads\\Nam 3\\Nhap mon KHDL\\Lab01_DL211122\\test.ipynb Cell 26\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Downloads/Nam%203/Nhap%20mon%20KHDL/Lab01_DL211122/test.ipynb#X34sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# TEST\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Downloads/Nam%203/Nhap%20mon%20KHDL/Lab01_DL211122/test.ipynb#X34sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39massert\u001b[39;00m df_split_players_info[\u001b[39m'\u001b[39m\u001b[39mRelease Clause\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mto_list()[:\u001b[39m5\u001b[39m] \u001b[39m==\u001b[39m [\u001b[39m366700000.0\u001b[39m, \u001b[39m198900000.0\u001b[39m, \u001b[39m172200000.0\u001b[39m, \u001b[39m131199999.99999999\u001b[39m, \u001b[39m99900000.0\u001b[39m]\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Downloads/Nam%203/Nhap%20mon%20KHDL/Lab01_DL211122/test.ipynb#X34sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39massert\u001b[39;00m df_split_players_info[\u001b[39m'\u001b[39m\u001b[39mRelease Clause\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mto_list()[\u001b[39m-\u001b[39m\u001b[39m5\u001b[39m:] \u001b[39m==\u001b[39m [\u001b[39m0.0\u001b[39m, \u001b[39m17100000.0\u001b[39m, \u001b[39m38500000.0\u001b[39m, \u001b[39m48400000.0\u001b[39m, \u001b[39m35500000.0\u001b[39m]\n",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# TEST\n",
    "assert df_split_players_info['Release Clause'].to_list()[:5] == [366700000.0, 198900000.0, 172200000.0, 131199999.99999999, 99900000.0]\n",
    "assert df_split_players_info['Release Clause'].to_list()[-5:] == [0.0, 17100000.0, 38500000.0, 48400000.0, 35500000.0]"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "a42ccb73e7d9bfdf27e036f1d2b8b681e55fc0743cc5586bc2474d4a60f4b886"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
